{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8d3d7e",
   "metadata": {},
   "source": [
    "# Learning to predict named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93094b9c",
   "metadata": {},
   "source": [
    "I went to <span style=\"border:#F87171 1px solid;border-radius:10px;padding:5px;padding-right:0px\">Cologne&nbsp;<span style=\"color:#991B1B;border:#F87171 1px solid;background-color:#FEE2E2;border-radius:10px;padding:5px\">CITY</span></span> yesterday. It was really nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f56ea2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fff9c",
   "metadata": {},
   "source": [
    "Let's say, we have a sentence like `I went to Cologne yesterday. It was really nice!` and, we want to extract that `{\"CITY\": \"Cologne\"}`. For such tasks, named entity recognition (NER) is your go-to solution. In this notebook, we'll load an already labeled text corpus and build a NER classifier using [embedders](https://github.com/code-kern-ai/embedders) and [sequence-learn](https://github.com/code-kern-ai/sequence-learn).\n",
    "\n",
    "As always, first, we got to import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfeda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.samples import get_entities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80197354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequencelearn.sequence_tagger import CRFTagger\n",
    "from sequencelearn.point_tagger import TreeTagger\n",
    "\n",
    "from sequencelearn.metrics import get_confusion_matrix\n",
    "from embedders.extraction.contextual import TransformerTokenEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8bdc7",
   "metadata": {},
   "source": [
    "Once we did so, we can load the sample data. We'll just grab 200 samples for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0490ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = get_entities_data(num_samples=200)\n",
    "print(corpus[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85bcbb0",
   "metadata": {},
   "source": [
    "Now, for NER to work well, we want to calculate tokens of our data. A token is e.g. a word, e.g. if you would split sentences at each whitespace; of course, there are cases in which tokenization is more complex, but for now, we can think of it like that.\n",
    "\n",
    "Further, we want to use modern, pre-trained architectures, to kickstart our models' performance. We will use transformers to calculate embeddings. With the `embedders` library, we provide a library that you can easily use to tokenize, embed, and lastly match your documents. This way, we can create highly informative token-level embeddings within one line of code. `\"distilbert-base-uncased\"` is the configuration string of the [transformer](https://huggingface.co/) model we want to load, `\"en_core_web_sm\"` is the language model of [spaCy](https://spacy.io/) that we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cce622",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TransformerTokenEmbedder(\"distilbert-base-uncased\", \"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0f9cf",
   "metadata": {},
   "source": [
    "Next, we can just pour our text corpus into the embedder and create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedder.fit_transform(corpus) \n",
    "# for pre-trained models, you can also just go with embedder.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3b0af",
   "metadata": {},
   "source": [
    "Now that we got our embeddings, we can specify a small amount of training samples. For now, we'll go with 100 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 100\n",
    "\n",
    "embeddings_train = embeddings[:num_train_samples]\n",
    "embeddings_test = embeddings[num_train_samples:]\n",
    "\n",
    "labels_train = labels[:num_train_samples]\n",
    "labels_test = labels[num_train_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df68e5",
   "metadata": {},
   "source": [
    "Now that the data is prepared, we can instantiate our model. In this example, we'll use a `TreeTagger`, a very simple approach predicting the sequence independently. There is also the option to use the `CRFTagger`, which is commonly used to predict labels for sequences when predictions are dependent on one another (i.e. there are different label probabilities for a given token $i$ depending on the label of token $i-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = TreeTagger()\n",
    "tagger.fit(embeddings_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33395f97",
   "metadata": {},
   "source": [
    "With our model instantiated and trained, we can now make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d242f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = tagger.predict(embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a9d16",
   "metadata": {},
   "source": [
    "Of course, we want to see how well our model predicts. We can just put our predictions and labels into the confusion matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, labels_sorted_bio = get_confusion_matrix(preds_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87be2b0",
   "metadata": {},
   "source": [
    "To help us analyze the results, we can make use of `\"ConfusionMatrixDisplay\"` from scikit-learn, so that we can see pairwise prediction/label combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_sorted_bio)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6699361",
   "metadata": {},
   "source": [
    "And that's it; you now got your tagger prepared, and can easily use it to predict named entities within texts.\n",
    "\n",
    "If you like our tutorial and the library, please consider giving this repository a star, or enter an issue for things you desire in this library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
